---
title: "Statistical analysis of significantly mutated genes"
author: "Collin Tokheim"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{Statistical analysis of significantly mutated genes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Identifying genes with more mutations then expected has been central methodology for identifying putative cancer driver genes in exome sequencing studies of cancer samples. Identifying significantly mutated genes fundamentally relies on estimating a background mutation rate. Mutation rate varies over more than 2 orders of magnitude providing a substantial statistical estimation challenge. Analysis not accounting for the uncertainty in mutation rate yields overly optimistic assessments. In this package, we examine statistical power (either with known or uncertain mutation rate) and false positives induced by unaccounted variation in mutation rate.

## Sample Size Calculation

An important aspect of designing cancer exome seqeuncing studies is to determine how many cancer samples are required for sufficient power to detect significantly mutated genes present at a certain prevalence.

### Assuming a known mutation rate

In general the mutation rate is not precisely known, but could be assumed to be known for the sake of power calculations. This results in an overly optimistic assessment of the required number of cancer samples. In the known mutation rate scenario, an exact binomial power calculation is performed.

```{r, fig.show='hold'}
library(cancerSeqStudy)

# setup parameters
samp.sizes <- seq(100, 4000, by=100)
desired.power <- .9
eff.size <- .02  # fraction of samples above background
mut.rate <- 1e-5
alpha.level <- 5e-6  # roughly a bonferoni corrected significance level

binomRequiredSampleSize(desired.power, mut.rate, samp.sizes, eff.size, alpha.level)
```

### Accounting for uncertain mutation rate

Adjusting for uncertainty in mutation rate better represents the actual required number of sequenced samples. To handle uncertain mutation rates (with a certain coefficient of variation), a beta-binomial power test is performed.

```{r, fig.show='hold'}
# setup parameters
cv <- .2  # coefficient of variation for mutation rate

bbdRequiredSampleSize(desired.power, mut.rate, cv, samp.sizes, eff.size)
```

Notice the minimum required samples raised from 1,500 to 3,500 by accounting for uncertainty of mutation rate with a coefficient of variation of .2.

## Calculating powered effect size

If you already have a certain number of samples, often it is helpful to understand the extent to which rare significantly mutated genes are characterized. 

```{r, fig.show='hold'}
# setup parameters
possible.eff.sizes <- seq(.01, .2, by=.01)  # fraction of samples above background
num.samples <- 1000  # number of samples in study

bbdPoweredEffectSize(possible.eff.sizes, desired.power, mut.rate, cv, num.samples, alpha.level)
```

In this example, drivers present in 4% of samples above background mutation rate have sufficient power to be detected.

## Expected false positives

```{r, fig.show='hold'}
bbd.params <- rateCvToAlphaBeta(mut.rate, cv)
binom.false.pos(bbd.params$alpha, bbd.params$beta, samp.sizes)
```

## Systematically examining power and false postives

To fully understand the effects on power and false positives, a variable sweep over a grid of potential values can be done. This is best done in parallel on a server with multiple cores. Reducing the number of evaluate mutation rates or the effective number of sample sizes evaluated will substantially increase speed, but will provide lower resolution on the shape of statistical power and false positives. One approach is to download the source files from github and run cancerSeqStudy.R as a script.

```{r, engine = 'bash', eval = FALSE}
$ cd cancerSeqStudy
$ Rscript R/cancerSeqStudy.R -c 10 -o myoutput.txt
```

Where `-c` expressess the number of cores to use, and `-o` designates the output file name. To change the parameters which are evaluated requires changing the cancerSeqStudy script. Alternatively, cancerSeqStudy may be installed and can be run with creating a new R file that uses the installed library. An extensive parameter sweep is shown below.

```{r, eval=FALSE}
library(cancerSeqStudy)
library(reshape2)
library(parallel)

#############################
# running options
#############################
output <- "statistical_analysis_output.txt"  # file name to save output
num.cores <- 1  # increase on multi-core computer to run faster!!!

#############################
# define the model params
#############################
# long list of mutation rates to be evaluated
rate <- c(.1e-6, .2e-6, .3e-6, .4e-6, .5e-6, .7e-6, .8e-6, 1e-6, 1.25e-6, 1.5e-6, 
          1.75e-6, 2e-6, 2.25e-6, 2.5e-6, 2.75e-6, 3e-6, 3.5e-6, 4e-6, 4.5e-6, 5e-6, 
          5.5e-6, 6e-6, 6.5e-6, 7e-6, 7.5e-6, 8e-6, 8.5e-6, 9e-6, 10e-6, 11e-6, 12e-6)
fg <- 3.9  # an adjustment factor that lawrence et al used for variable gene length
rate <- fg*rate  # nominal rates are adjusted (will have to adjust back after analysis is done)

# model parameters
nonsilentFactor <- 3/4  # roughly the fraction of mutations that are non-silent
L <- 1500  # same length as used in lawrence et al. paper
Leff <- L * nonsilentFactor
desired.power <- .9  # aka 90% power
possible.cvs <- c(.05, .1, .2)  # coefficient of variation for mutation rate per base
effect.sizes <- c(.01, .02, .05)  # fraction of samples above background
alpha.levels <- c(1e-4, 5e-6)  # level of significance

# setting up the sample sizes to check
N <- 25000
by.step <- 25
samp.sizes <- seq(by.step, N, by=by.step)  # grid of sample sizes to check

##################################
# Loop through different params
##################################
param.list <- list()
counter <- 1
for (i in 1:length(rate)){
  # loop over effect sizes
  for (effect.size in effect.sizes){
    # loop over alpha levels
    for (alpha.level in alpha.levels){
      param.list[[counter]] <- c(rate[i], effect.size, alpha.level)
      counter <- counter + 1
    }
  }
}

############################
# run analysis
############################
result.list <- mclapply(param.list, runAnalysisList, mc.cores=num.cores,
                        samp.sizes=samp.sizes, desired.power=desired.power,
                        Leff=Leff, possible.cvs=possible.cvs)
result.df <- do.call("rbind", result.list)

# adjust mutation rates back to the average
result.df$mutation.rate <- result.df$mutation.rate / fg
# convert to factor
result.df$mutation.rate <- factor(result.df$mutation.rate, levels=unique(result.df$mutation.rate))
result.df$effect.size <- factor(result.df$effect.size, levels=unique(result.df$effect.size))

######################
# Save result to text file
######################
write.table(result.df, output, sep='\t')
```

